{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Visitors DW\n",
    "__Supporting officials' decision-making to provide better visitors experience in the US__\n",
    "***\n",
    "## Overview\n",
    "The purpose of this data engineering capstone project is to give students a chance to combine what they've learned throughout the program. This project will be an important part of learners portfolio that will help to achieve data engineering-related career goals. We could choose to complete the project provided by the Udacity team or define the scope and data ourselves. I took the first approach in building the DW on the data on immigration to the United States provided by Udacity.\n",
    "\n",
    "## Business Scenario\n",
    "We are D2I (Data to Insights), a business consulting firm specialized in data warehouse services through assisting the enterprises with navigating their data needs and creating strategic operational solutions that deliver tangible business results. Specifically, we can help with the modernization of corporations' data warehousing infrastructure by improving performance and ease of use for end users, enhancing functionality, decreasing total cost of ownership while making it possible for real-time decision making. In total, our full suite of services includes helping enterprises with data profiling, data standardization, data acquisition, data transformation and integration.\n",
    "\n",
    "We have been contracted by the U.S. Customs and Border Protection to help them see what is hidden behind the data flood. We aim to model and create a brand new analytics solution on top of the state-of-the-art technolgies available to enable them to unleash insights from data then providing better customer experiences when coming to the US.\n",
    "\n",
    "## Structure of the Project\n",
    "Following the Udacity guide for this project, we structured this documentation with steps below:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### The Scope \n",
    "_Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>_\n",
    "\n",
    "The main deliverable of our work here will be a data warehouse in the cloud that will support answering questions through analytics tables and dashboards. Additionally, as we developed a general source-of-truth database, the Government of the US could open the solution through a web API so backend web services could query the warehouse for information relating to international visitors.\n",
    "\n",
    "The main information and questions a user may want to extract from the data mart would be:\n",
    "\n",
    "* Visitors by world regions.\n",
    "* Visitors by demographics.\n",
    "* Correlations between destination and source demographics.\n",
    "* Correlations between destination and source climates.\n",
    "* Correlations between immigration by source region, and the source region temperature.\n",
    "* Correlations between visitor demographics, and states visited.\n",
    "\n",
    "***\n",
    "__IMMIGRATION DATA__\n",
    "\n",
    "For decades, U.S. immigration officers issued the I-94 Form (Arrival/Departure Record) to foreign visitors (e.g., business visitors, tourists and foreign students) who lawfully entered the United States. The I-94 was a small white paper form that a foreign visitor received from cabin crews on arrival flights and from U.S. Customs and Border Protection at the time of entry into the United States. It listed the traveler's immigration category, port of entry, data of entry into the United States, status expiration date and had a unique 11-digit identifying number assigned to it. Its purpose was to record the traveler's lawful admission to the United States.\n",
    "\n",
    "This is the main dataset and there is a file for each month of the year of 2016 available in the directory `../../data/18-83510-I94-Data-2016/` in the [SAS](https://www.sas.com/en_us/home.html) binary database storage format `sas7bdat`. Combined, the 12 datasets have got more than 40 million rows (40.790.529) and 28 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some EDA (exploratory data analysis) using just the the month of April. The related dataset has more than three million records (3.096.313).\n",
    "\n",
    "__Some key-words to use:__\n",
    "\n",
    "Redshift: Massively Parallel, column-oriented\n",
    "Break the large files up into smaller files and ingest them to Redshift in parallel. This is the way COPY command works. S3 and Redshift in the same region in order to avoid big latency times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed in this project\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from helper.util import convert_sas_date, convert_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration = pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "#immigration = pd.read_csv(\"immigration_data_sample.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>fltno</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*FF</td>\n",
       "      <td>00001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7C</td>\n",
       "      <td>3154</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7C</td>\n",
       "      <td>31545</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7C</td>\n",
       "      <td>31554</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7C</td>\n",
       "      <td>34020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7C</td>\n",
       "      <td>34022</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7C</td>\n",
       "      <td>34023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7C</td>\n",
       "      <td>3402C</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHP</td>\n",
       "      <td>00490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7C</td>\n",
       "      <td>34043</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7C</td>\n",
       "      <td>34047</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7C</td>\n",
       "      <td>34052</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CH</td>\n",
       "      <td>00490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7C</td>\n",
       "      <td>34504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7C</td>\n",
       "      <td>36404</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CH</td>\n",
       "      <td>00456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CEY</td>\n",
       "      <td>00420</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7C</td>\n",
       "      <td>72667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RS</td>\n",
       "      <td>7667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7C</td>\n",
       "      <td>31024</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7C</td>\n",
       "      <td>31016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7C</td>\n",
       "      <td>07602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>YF</td>\n",
       "      <td>15520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7C</td>\n",
       "      <td>03420</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7C</td>\n",
       "      <td>03541</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7C</td>\n",
       "      <td>03602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7C</td>\n",
       "      <td>03604</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7C</td>\n",
       "      <td>03702</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7C</td>\n",
       "      <td>03706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7C</td>\n",
       "      <td>03802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10126</th>\n",
       "      <td>JL</td>\n",
       "      <td>00941</td>\n",
       "      <td>6682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10127</th>\n",
       "      <td>HA</td>\n",
       "      <td>00458</td>\n",
       "      <td>6736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10128</th>\n",
       "      <td>KL</td>\n",
       "      <td>00641</td>\n",
       "      <td>6826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129</th>\n",
       "      <td>JL</td>\n",
       "      <td>00782</td>\n",
       "      <td>6880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10130</th>\n",
       "      <td>KL</td>\n",
       "      <td>00605</td>\n",
       "      <td>6968.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10131</th>\n",
       "      <td>JL</td>\n",
       "      <td>00792</td>\n",
       "      <td>7219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10132</th>\n",
       "      <td>KE</td>\n",
       "      <td>00113</td>\n",
       "      <td>7546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10133</th>\n",
       "      <td>OZ</td>\n",
       "      <td>00625</td>\n",
       "      <td>7585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134</th>\n",
       "      <td>AF</td>\n",
       "      <td>00084</td>\n",
       "      <td>7730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10135</th>\n",
       "      <td>LH</td>\n",
       "      <td>00464</td>\n",
       "      <td>7776.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136</th>\n",
       "      <td>KE</td>\n",
       "      <td>00053</td>\n",
       "      <td>7811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10137</th>\n",
       "      <td>LH</td>\n",
       "      <td>00454</td>\n",
       "      <td>7823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>LH</td>\n",
       "      <td>00456</td>\n",
       "      <td>7881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139</th>\n",
       "      <td>LH</td>\n",
       "      <td>00400</td>\n",
       "      <td>7904.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10140</th>\n",
       "      <td>BA</td>\n",
       "      <td>00287</td>\n",
       "      <td>7927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10141</th>\n",
       "      <td>AF</td>\n",
       "      <td>00090</td>\n",
       "      <td>7938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>BA</td>\n",
       "      <td>00275</td>\n",
       "      <td>7951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10143</th>\n",
       "      <td>JL</td>\n",
       "      <td>00794</td>\n",
       "      <td>7983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144</th>\n",
       "      <td>QF</td>\n",
       "      <td>00011</td>\n",
       "      <td>8041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145</th>\n",
       "      <td>QF</td>\n",
       "      <td>00093</td>\n",
       "      <td>8070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10146</th>\n",
       "      <td>JJ</td>\n",
       "      <td>08094</td>\n",
       "      <td>8230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10147</th>\n",
       "      <td>BA</td>\n",
       "      <td>00283</td>\n",
       "      <td>8296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10148</th>\n",
       "      <td>AF</td>\n",
       "      <td>00066</td>\n",
       "      <td>8519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10149</th>\n",
       "      <td>JJ</td>\n",
       "      <td>08090</td>\n",
       "      <td>8808.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10150</th>\n",
       "      <td>VS</td>\n",
       "      <td>00015</td>\n",
       "      <td>9305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10151</th>\n",
       "      <td>LH</td>\n",
       "      <td>00462</td>\n",
       "      <td>9830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10152</th>\n",
       "      <td>VS</td>\n",
       "      <td>00075</td>\n",
       "      <td>10300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>VS</td>\n",
       "      <td>00027</td>\n",
       "      <td>10387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10154</th>\n",
       "      <td>AF</td>\n",
       "      <td>00006</td>\n",
       "      <td>10472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10155</th>\n",
       "      <td>VS</td>\n",
       "      <td>00043</td>\n",
       "      <td>11001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline  fltno    count\n",
       "0         *FF  00001      1.0\n",
       "1          7C   3154      1.0\n",
       "2          7C  31545      1.0\n",
       "3          7C  31554      1.0\n",
       "4          7C  34020      1.0\n",
       "5          7C  34022      1.0\n",
       "6          7C  34023      1.0\n",
       "7          7C  3402C      1.0\n",
       "8         CHP  00490      1.0\n",
       "9          7C  34043      1.0\n",
       "10         7C  34047      1.0\n",
       "11         7C  34052      1.0\n",
       "12         CH  00490      1.0\n",
       "13         7C  34504      1.0\n",
       "14         7C  36404      1.0\n",
       "15         CH  00456      1.0\n",
       "16        CEY  00420      1.0\n",
       "17         7C  72667      1.0\n",
       "18         RS   7667      1.0\n",
       "19         7C  31024      1.0\n",
       "20         7C  31016      1.0\n",
       "21         7C  07602      1.0\n",
       "22         YF  15520      1.0\n",
       "23         7C  03420      1.0\n",
       "24         7C  03541      1.0\n",
       "25         7C  03602      1.0\n",
       "26         7C  03604      1.0\n",
       "27         7C  03702      1.0\n",
       "28         7C  03706      1.0\n",
       "29         7C  03802      1.0\n",
       "...       ...    ...      ...\n",
       "10126      JL  00941   6682.0\n",
       "10127      HA  00458   6736.0\n",
       "10128      KL  00641   6826.0\n",
       "10129      JL  00782   6880.0\n",
       "10130      KL  00605   6968.0\n",
       "10131      JL  00792   7219.0\n",
       "10132      KE  00113   7546.0\n",
       "10133      OZ  00625   7585.0\n",
       "10134      AF  00084   7730.0\n",
       "10135      LH  00464   7776.0\n",
       "10136      KE  00053   7811.0\n",
       "10137      LH  00454   7823.0\n",
       "10138      LH  00456   7881.0\n",
       "10139      LH  00400   7904.0\n",
       "10140      BA  00287   7927.0\n",
       "10141      AF  00090   7938.0\n",
       "10142      BA  00275   7951.0\n",
       "10143      JL  00794   7983.0\n",
       "10144      QF  00011   8041.0\n",
       "10145      QF  00093   8070.0\n",
       "10146      JJ  08094   8230.0\n",
       "10147      BA  00283   8296.0\n",
       "10148      AF  00066   8519.0\n",
       "10149      JJ  08090   8808.0\n",
       "10150      VS  00015   9305.0\n",
       "10151      LH  00462   9830.0\n",
       "10152      VS  00075  10300.0\n",
       "10153      VS  00027  10387.0\n",
       "10154      AF  00006  10472.0\n",
       "10155      VS  00043  11001.0\n",
       "\n",
       "[10156 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration[immigration.i94mode == 1].groupby([\"airline\", \"fltno\"])[\"count\"].sum(dropna=False).sort_values(ascending = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'immigration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6943f2e62ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimmigration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'immigration' is not defined"
     ]
    }
   ],
   "source": [
    "immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration = convert_integer(immigration, ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', \\\n",
    "                              'arrdate', 'i94mode', 'i94bir', 'i94visa', 'count', 'biryear', 'dtadfile', 'depdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = dict(zip(pd.read_csv(\"lookup/I94PORT.csv\").to_dict(\"list\")[\"ID\"], pd.read_csv(\"lookup/I94PORT.csv\").to_dict(\"list\")[\"Port\"]))\n",
    "immigration[\"i94port_desc\"] = immigration[\"i94port\"].map(port, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = dict(zip(pd.read_csv(\"lookup/I94CIT_I94RES.csv\").to_dict(\"list\")[\"Code\"], pd.read_csv(\"lookup/I94CIT_I94RES.csv\").to_dict(\"list\")[\"I94CTRY\"]))\n",
    "immigration[\"i94cit_desc\"] = immigration[\"i94cit\"].map(countries, na_action='ignore')\n",
    "immigration[\"i94res_desc\"] = immigration[\"i94res\"].map(countries, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = dict(zip(pd.read_csv(\"lookup/I94MODE.csv\").to_dict(\"list\")[\"ID\"], pd.read_csv(\"lookup/I94MODE.csv\").to_dict(\"list\")[\"Mode\"]))\n",
    "immigration[\"i94mode_desc\"] = immigration[\"i94mode\"].map(modes, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "addrs = dict(zip(pd.read_csv(\"lookup/I94ADDR.csv\").to_dict(\"list\")[\"Code\"], pd.read_csv(\"lookup/I94ADDR.csv\").to_dict(\"list\")[\"State\"]))\n",
    "immigration[\"i94addr_desc\"] = immigration[\"i94addr\"].map(addrs, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "visas = dict(zip(pd.read_csv(\"lookup/I94VISA.csv\").to_dict(\"list\")[\"ID\"], pd.read_csv(\"lookup/I94VISA.csv\").to_dict(\"list\")[\"Type\"]))\n",
    "immigration[\"i94visa_desc\"] = immigration[\"i94visa\"].map(visas, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Dictionary__: Here, we describe the various fields of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| CICID* | ID that uniquely identify one record in the dataset |\n",
    "| I94YR | 4 digit year |\n",
    "| I94MON | Numeric month |\n",
    "| I94CIT | 3 digit code of source city for immigration (Born in) |\n",
    "| I94RES | 3 digit code of source country for immigration (Residence in) |\n",
    "| I94PORT | Port addmitted through |\n",
    "| ARRDATE | Arrival date in the USA |\n",
    "| I94MODE | Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) |\n",
    "| I94ADDR | State of arrival |\n",
    "| DEPDATE | Departure date |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student) |\n",
    "| COUNT | Used for summary statistics |\n",
    "| DTADFILE | Character Date Field |\n",
    "| VISAPOST | Department of State where where Visa was issued |\n",
    "| OCCUP | Occupation that will be performed in U.S. |\n",
    "| ENTDEPA | Arrival Flag. Whether admitted or paroled into the US |\n",
    "| ENTDEPD | Departure Flag. Whether departed, lost visa, or deceased |\n",
    "| ENTDEPU | Update Flag. Update of visa, either apprehended, overstayed, or updated to PR |\n",
    "| MATFLAG | Match flag |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| DTADDTO | Character date field to when admitted in the US |\n",
    "| GENDER | Gender |\n",
    "| INSNUM | INS number |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| ADMNUM | Admission number, should be unique and not nullable |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "__Global Temperature Data__\n",
    "\n",
    "There are a range of organizations that collate climate trends data. The three most cited land and ocean temperature data sets are NOAA’s MLOST, NASA’s GISTEMP and the UK’s HadCrut.\n",
    "\n",
    "The Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory, has repackaged the data from a newer compilation put it all together. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.\n",
    "\n",
    "In the original dataset from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data), several files are available but in this capstone project we will be using only the `GlobalLandTemperaturesByCity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "world_temperature = pd.read_csv(temperature_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| dt | Date in format YYYY-MM-DD |\n",
    "| AverageTemperature | Average temperature of the city in a given date |\n",
    "| City | City Name |\n",
    "| Country | Country Name |\n",
    "| Latitude | Latitude |\n",
    "| Longitude | Longitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides a long period of the world's temperature (from year 1743 to 2013). However, since the immigration dataset only has data of the US National Tourism Office in the year of 2016, the vast majority of the data here is useless. We are only keeping the American cities' latitude and longitude fields to form a dimension table for cities. It would be interesting if we could cross the two tables in order to analyse how the waves of immigration to the US relate to the changes in the temperature. But this is just unfeasible due to the different dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_temperature = world_temperature.groupby([\"Country\", \"City\"]).agg({\"AverageTemperature\": \"mean\", \n",
    "                                                                        \"Latitude\": \"first\", \"Longitude\": \"first\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Baglan</td>\n",
       "      <td>10.790278</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>69.61E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Gardez</td>\n",
       "      <td>17.274240</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>69.89E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Gazni</td>\n",
       "      <td>10.311996</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>67.98E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Herat</td>\n",
       "      <td>14.213004</td>\n",
       "      <td>34.56N</td>\n",
       "      <td>62.27E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Jalalabad</td>\n",
       "      <td>14.342919</td>\n",
       "      <td>34.56N</td>\n",
       "      <td>70.05E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country       City  AverageTemperature Latitude Longitude\n",
       "0  Afghanistan     Baglan           10.790278   36.17N    69.61E\n",
       "1  Afghanistan     Gardez           17.274240   32.95N    69.89E\n",
       "2  Afghanistan      Gazni           10.311996   32.95N    67.98E\n",
       "3  Afghanistan      Herat           14.213004   34.56N    62.27E\n",
       "4  Afghanistan  Jalalabad           14.342919   34.56N    70.05E"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Airports Data__\n",
    "\n",
    "The airport codes may refer to either [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the [ICAO](https://en.wikipedia.org/wiki/ICAO_airport_code) airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia).\n",
    "\n",
    "Airport codes from around the world. Downloaded from public domain source http://ourairports.com/data/ who compiled this data from multiple different sources.\n",
    "\n",
    "`airport-codes.csv` contains the list of all airport codes, the attributes are identified in datapackage description. Some of the columns contain attributes identifying airport locations, other codes (IATA, local if exist) that are relevant to identification of an airport.\n",
    "Original source url is http://ourairports.com/data/airports.csv (stored in archive/data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = pd.read_csv(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18555</th>\n",
       "      <td>EGLL</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>London Heathrow Airport</td>\n",
       "      <td>83.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB-ENG</td>\n",
       "      <td>London</td>\n",
       "      <td>EGLL</td>\n",
       "      <td>LHR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.461941, 51.4706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ident           type                     name  elevation_ft continent  \\\n",
       "18555  EGLL  large_airport  London Heathrow Airport          83.0        EU   \n",
       "\n",
       "      iso_country iso_region municipality gps_code iata_code local_code  \\\n",
       "18555          GB     GB-ENG       London     EGLL       LHR        NaN   \n",
       "\n",
       "              coordinates  \n",
       "18555  -0.461941, 51.4706  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport[airport.ident == \"EGLL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = pd.read_csv(\"airports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude_deg</th>\n",
       "      <th>longitude_deg</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>scheduled_service</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>home_link</th>\n",
       "      <th>wikipedia_link</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, ident, type, name, latitude_deg, longitude_deg, elevation_ft, continent, iso_country, iso_region, municipality, scheduled_service, gps_code, iata_code, local_code, home_link, wikipedia_link, keywords]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__U.S. City Demographic Data__\n",
    "\n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "This product uses the Census Bureau Data API but is not endorsed or certified by the Census Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_demographics = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper.etl as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = e.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#immigration = e.etl_immigration_data(spark, load_size=10)\n",
    "#immigration = e.etl_immigration_data(spark, load_size=10, output_path=e.OUTPUT + \"immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration = e.etl_immigration_data(spark, input_path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', \n",
    "                              input_format = \"com.github.saurfang.sas.spark\", load_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-------+------+----------+-----+----------+-------+-----+-------+-------+--------+------+------+------+----+\n",
      "|i94addr|i94mon|cicid|i94visa|i94res|   arrdate|i94yr|   depdate|airline|fltno|i94mode|i94port|visatype|gender|i94cit|i94bir|stay|\n",
      "+-------+------+-----+-------+------+----------+-----+----------+-------+-----+-------+-------+--------+------+------+------+----+\n",
      "|   null|     4|    6|      2|   692|2016-04-29| 2016|      null|   null| null|   null|    XXX|      B2|  null|   692|    37|null|\n",
      "|     AL|     4|    7|      3|   276|2016-04-07| 2016|      null|   null|00296|      1|    ATL|      F1|     M|   254|    25|null|\n",
      "|     MI|     4|   15|      2|   101|2016-04-01| 2016|2016-08-25|     OS|   93|      1|    WAS|      B2|     M|   101|    55| 146|\n",
      "|     MA|     4|   16|      2|   101|2016-04-01| 2016|2016-04-23|     AA|00199|      1|    NYC|      B2|  null|   101|    28|  22|\n",
      "|     MA|     4|   17|      2|   101|2016-04-01| 2016|2016-04-23|     AA|00199|      1|    NYC|      B2|  null|   101|     4|  22|\n",
      "|     MI|     4|   18|      1|   101|2016-04-01| 2016|2016-04-11|     AZ|00602|      1|    NYC|      B1|  null|   101|    57|  10|\n",
      "|     NJ|     4|   19|      2|   101|2016-04-01| 2016|2016-04-14|     AZ|00602|      1|    NYC|      B2|  null|   101|    63|  13|\n",
      "|     NJ|     4|   20|      2|   101|2016-04-01| 2016|2016-04-14|     AZ|00602|      1|    NYC|      B2|  null|   101|    57|  13|\n",
      "|     NY|     4|   21|      2|   101|2016-04-01| 2016|2016-04-09|     AZ|00602|      1|    NYC|      B2|  null|   101|    46|   8|\n",
      "|     NY|     4|   22|      1|   101|2016-04-01| 2016|2016-04-18|     AZ|00608|      1|    NYC|      B1|  null|   101|    48|  17|\n",
      "+-------+------+-----+-------+------+----------+-----+----------+-------+-----+-------+-------+--------+------+------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = e.etl_countries_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+--------+---------+\n",
      "|Code|             Country|       Temperature|Latitude|Longitude|\n",
      "+----+--------------------+------------------+--------+---------+\n",
      "| 532|               Aruba|              null|    null|     null|\n",
      "| 110|             Finland| 3.711644535691719|  60.27N|   25.95E|\n",
      "| 438|           Australia| 16.70146214247643|  34.56S|  138.16E|\n",
      "| 113|              Greece|16.347482714233163|  37.78N|   24.41E|\n",
      "| 126|            Portugal|14.749674965924589|  39.38N|    8.32W|\n",
      "| 737|Invalid: Midway I...|              null|    null|     null|\n",
      "| 251|              Israel|19.007696591476424|  31.35N|   33.93E|\n",
      "| 849|No Country Code (...|              null|    null|     null|\n",
      "| 162|             Ukraine| 7.822183953883213|  49.03N|   29.39E|\n",
      "| 508|Netherlands Antilles|              null|    null|     null|\n",
      "+----+--------------------+------------------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = e.etl_states_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------------+---------------+-------+-----------+-----------------------------+----------------+--------------+------+----------------+--------------+\n",
      "|Code|               State|BlackOrAfrican-American|TotalPopulation|  White|ForeignBorn|AmericanIndianAndAlaskaNative|HispanicOrLatino|NumberVeterans| Asian|FemalePopulation|MalePopulation|\n",
      "+----+--------------------+-----------------------+---------------+-------+-----------+-----------------------------+----------------+--------------+------+----------------+--------------+\n",
      "|  AZ|             Arizona|                 296222|        4499542|3591611|     682313|                       129708|         1508157|        264505|229183|         2272087|       2227455|\n",
      "|  SC|      South Carolina|                 175064|         533657| 343764|      27744|                         3705|           29863|         33463| 13355|          272713|        260944|\n",
      "|  LA|           Louisiana|                 602377|        1300595| 654578|      83419|                         8263|           87133|         69771| 38739|          673597|        626998|\n",
      "|  MN|           Minnesota|                 216731|        1422403|1050239|     215873|                        25242|          103229|         64894|151544|          720246|        702157|\n",
      "|  NJ|          New Jersey|                 452202|        1428908| 615083|     477028|                        11350|          600437|         30195|116844|          723172|        705736|\n",
      "|  DC|District of Columbia|                 328786|         672228| 285402|      95117|                         6130|           71129|         25963| 35072|          352523|        319705|\n",
      "|  OR|              Oregon|                  72150|        1436509|1235819|     185753|                        38597|          201498|         78948|117279|          729066|        707443|\n",
      "|  99|     All Other Codes|                   null|           null|   null|       null|                         null|            null|          null|  null|            null|          null|\n",
      "|  VA|            Virginia|                 771569|        2363622|1428158|     269254|                        26160|          216760|        229766|167784|         1203148|       1160474|\n",
      "|  RI|        Rhode Island|                  55556|         413562| 287304|      87365|                         6369|          109226|         18607| 24245|          210746|        202816|\n",
      "+----+--------------------+-----------------------+---------------+-------+-----------+-----------------------------+----------------+--------------+------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  data-engineer-capstone\n",
      "  elasticbeanstalk-us-east-1-900646315604\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# Retrieve the list of existing buckets\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bucket(\"y435\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
